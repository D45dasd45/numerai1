{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SHAJEqyUSymu",
        "outputId": "88b3073c-4371-4af4-804d-307a5c8e5509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in c:\\users\\neyo_\\appdata\\roaming\\python\\python311\\site-packages (23.3.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\neyo_\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
            "  warnings.warn(\n",
            "2023-12-12 21:21:28,952 INFO numerapi.utils: target file already exists\n",
            "2023-12-12 21:21:28,952 INFO numerapi.utils: download complete\n",
            "2023-12-12 21:21:30,824 INFO numerapi.utils: target file already exists\n",
            "2023-12-12 21:21:30,825 INFO numerapi.utils: download complete\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 10660\n",
            "[LightGBM] [Info] Number of data points in the train set: 2420521, number of used features: 2132\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 2132 dense feature groups (2465.36 MB) transferred to GPU in 4.066455 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.500015\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "%pip install --upgrade pip\n",
        "%pip install -q numerapi pandas pyarrow matplotlib lightgbm scikit-learn cloudpickle seaborn scipy==1.10.1\n",
        "\n",
        "# Inline plots\n",
        "%matplotlib inline\n",
        "# %pip install colabcode\n",
        "# %pip install googlecolab\n",
        "# %pip install google\n",
        "\n",
        "\n",
        "\n",
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "import json\n",
        "from numerapi import NumerAPI\n",
        "\n",
        "\n",
        "\n",
        "# Download data\n",
        "napi = NumerAPI()\n",
        "napi.download_dataset(\"v4.2/train_int8.parquet\")\n",
        "napi.download_dataset(\"v4.2/features.json\")\n",
        "\n",
        "# Load data\n",
        "feature_metadata = json.load(open(\"v4.2/features.json\"))\n",
        "feature_cols = feature_metadata[\"feature_sets\"][\"all\"]\n",
        "target_cols = feature_metadata[\"targets\"]\n",
        "train = pd.read_parquet(\"v4.2/train_int8.parquet\", columns=[\"era\"] + feature_cols + target_cols)\n",
        "\n",
        "# Number of features \n",
        "len(feature_cols)\n",
        "\n",
        "# Print target columns\n",
        "train[[\"era\"] + target_cols]\n",
        "\n",
        "# Drop `target` column\n",
        "assert train[\"target\"].equals(train[\"target_cyrus_v4_20\"])\n",
        "target_names = target_cols[1:]\n",
        "targets_df = train[[\"era\"] + target_names]\n",
        "# Print target names grouped by name and time horizon\n",
        "pd.set_option('display.max_rows', 100)\n",
        "t20s = [t for t in target_names if t.endswith(\"_20\")]\n",
        "t60s = [t for t in target_names if t.endswith(\"_60\")]\n",
        "names = [t[7:-6] for t in t20s]\n",
        "pd.DataFrame({\"name\": names,\"20\": t20s,\"60\": t60s}).set_index(\"name\")\n",
        "targets_df[[\"target_cyrus_v4_20\", \"target_cyrus_v4_60\", \"target_xerxes_v4_20\", \"target_xerxes_v4_60\"]].plot(kind=\"hist\", bins=35, density=True, figsize=(8, 4), title=\"Target Distributions\", subplots=True, layout=(2, 2), ylabel=\"\", yticks=[]);\n",
        "# print number of NaNs per era\n",
        "nans_per_era = targets_df.groupby(\"era\").apply(lambda x: x.isna().sum())\n",
        "nans_per_era[target_names].plot(figsize=(64, 32), title=\"Number of NaNs per Era\", legend=False)\n",
        "# Plot correlation matrix of targets\n",
        "import seaborn as sns\n",
        "sns.heatmap(targets_df[target_names].corr(), cmap=\"coolwarm\", xticklabels=False, yticklabels=False)\n",
        "\n",
        "\n",
        "\n",
        "# Arbitrarily pick a few 20-day target candidates\n",
        "target_candidates = [\"target_cyrus_v4_20\", \"target_sam_v4_20\", \"target_caroline_v4_20\", \"target_xerxes_v4_20\"]\n",
        "\n",
        "targets_df[target_names].corrwith(targets_df[\"target_cyrus_v4_20\"]).sort_values(ascending=False).to_frame(\"corr_with_cyrus_v4_20\")\n",
        "\n",
        "\n",
        "models = {}\n",
        "for target in target_candidates:\n",
        "    model = lgb.LGBMRegressor(\n",
        "        device='gpu',\n",
        "        n_estimators=4000,\n",
        "        learning_rate=0.01,\n",
        "        max_depth=25,\n",
        "        num_leaves=31,\n",
        "        colsample_bytree=0.1,\n",
        "        force_col_wise=True,       \n",
        "     \n",
        "    )\n",
        "    model.fit(\n",
        "        train[feature_cols],\n",
        "        train[target]\n",
        "    )\n",
        "    models[target] = model\n",
        "    \n",
        "    PYDEVD_DISABLE_FILE_VALIDATION=1\n",
        "    # Download validation data\n",
        "napi.download_dataset(\"v4.2/validation_int8.parquet\")\n",
        "\n",
        "# Load the validation data, filtering for data_type == \"validation\"\n",
        "validation = pd.read_parquet(\"v4.2/validation_int8.parquet\", columns=[\"era\", \"data_type\"] + feature_cols + target_cols)\n",
        "validation = validation[validation[\"data_type\"] == \"validation\"]\n",
        "del validation[\"data_type\"]\n",
        "\n",
        "# Downsample every 4th era to reduce memory usage and speedup validation (suggested for Colab free tier)\n",
        "# Comment out the line below to use all the data\n",
        "# validation = validation[validation[\"era\"].isin(validation[\"era\"].unique()[::4])]\n",
        "\n",
        "# Embargo overlapping eras from training data\n",
        "last_train_era = int(train[\"era\"].unique()[-1])\n",
        "eras_to_embargo = [str(era).zfill(4) for era in [last_train_era + i for i in range(4)]]\n",
        "validation = validation[~validation[\"era\"].isin(eras_to_embargo)]\n",
        "\n",
        "# Generate validation predictions for each model\n",
        "for target in target_candidates:\n",
        "    validation[f\"prediction_{target}\"] = models[target].predict(validation[feature_cols])\n",
        "\n",
        "pred_cols = [f\"prediction_{target}\" for target in target_candidates]\n",
        "validation[pred_cols]\n",
        "\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "def numerai_corr(preds, target):\n",
        "    ranked_preds = (preds.rank(method=\"average\").values - 0.5) / preds.count()\n",
        "    gauss_ranked_preds = stats.norm.ppf(ranked_preds)\n",
        "    centered_target = target - target.mean()\n",
        "    preds_p15 = np.sign(gauss_ranked_preds) * np.abs(gauss_ranked_preds) ** 1.5\n",
        "    target_p15 = np.sign(centered_target) * np.abs(centered_target) ** 1.5\n",
        "    return np.corrcoef(preds_p15, target_p15)[0, 1]\n",
        "\n",
        "\n",
        "correlations = {}\n",
        "cumulative_correlations = {}\n",
        "for target in target_candidates:\n",
        "    correlations[f\"prediction_{target}\"] = validation.groupby(\"era\").apply(lambda d: numerai_corr(d[f\"prediction_{target}\"], d[\"target\"]))\n",
        "    cumulative_correlations[f\"prediction_{target}\"] = correlations[f\"prediction_{target}\"].cumsum()\n",
        "\n",
        "cumulative_correlations = pd.DataFrame(cumulative_correlations)\n",
        "cumulative_correlations.plot(title=\"Cumulative Correlation of validation Predictions\", figsize=(10, 6), xticks=[])\n",
        "\n",
        "summary_metrics = {}\n",
        "for target in target_candidates:\n",
        "    # per era correlation between this target and cyrus\n",
        "    mean_corr_with_cryus = validation.groupby(\"era\").apply(lambda d: d[target].corr(d[\"target_cyrus_v4_20\"])).mean()\n",
        "    # per era correlation between predictions of the model trained on this target and cyrus\n",
        "    mean = correlations[f\"prediction_{target}\"].mean()\n",
        "    std = correlations[f\"prediction_{target}\"].std()\n",
        "    sharpe = mean / std\n",
        "    rolling_max = cumulative_correlations[f\"prediction_{target}\"].expanding(min_periods=1).max()\n",
        "    max_drawdown = (rolling_max - cumulative_correlations[f\"prediction_{target}\"]).max()\n",
        "    summary_metrics[f\"prediction_{target}\"] = {\n",
        "        \"mean\": mean,\n",
        "        \"std\": std,\n",
        "        \"sharpe\": sharpe,\n",
        "        \"max_drawdown\": max_drawdown,\n",
        "        \"mean_corr_with_cryus\": mean_corr_with_cryus,\n",
        "    }\n",
        "pd.set_option('display.float_format', lambda x: '%f' % x)\n",
        "summary = pd.DataFrame(summary_metrics).T\n",
        "summary\n",
        "\n",
        "# Ensemble predictions together with a simple average\n",
        "favorite_targets = [\"target_cyrus_v4_20\", \"target_xerxes_v4_20\"]\n",
        "ensemble_cols = [f\"prediction_{target}\" for target in favorite_targets]\n",
        "validation[\"ensemble\"] = validation.groupby(\"era\")[ensemble_cols].rank(pct=True).mean(axis=1)\n",
        "\n",
        "# Print the ensemble predictions\n",
        "pred_cols = ensemble_cols + [\"ensemble\"]\n",
        "validation[pred_cols]\n",
        "\n",
        "correlations = {}\n",
        "cumulative_correlations = {}\n",
        "for col in pred_cols:\n",
        "    correlations[col] = validation.groupby(\"era\").apply(lambda d: numerai_corr(d[col], d[\"target\"]))\n",
        "    cumulative_correlations[col] = correlations[col].cumsum()\n",
        "\n",
        "cumulative_correlations = pd.DataFrame(cumulative_correlations)\n",
        "cumulative_correlations.plot(title=\"Cumulative Correlation of validation Predictions\", figsize=(10, 6), xticks=[])\n",
        "\n",
        "summary_metrics = {}\n",
        "for col in pred_cols:\n",
        "    mean = correlations[col].mean()\n",
        "    std = correlations[col].std()\n",
        "    sharpe = mean / std\n",
        "    rolling_max = cumulative_correlations[col].expanding(min_periods=1).max()\n",
        "    max_drawdown = (rolling_max - cumulative_correlations[col]).max()\n",
        "    summary_metrics[col] = {\n",
        "        \"mean\": mean,\n",
        "        \"std\": std,\n",
        "        \"sharpe\": sharpe,\n",
        "        \"max_drawdown\": max_drawdown,\n",
        "    }\n",
        "pd.set_option('display.float_format', lambda x: '%f' % x)\n",
        "summary = pd.DataFrame(summary_metrics).T\n",
        "summary\n",
        "\n",
        "def predict_ensemble(live_features: pd.DataFrame) -> pd.DataFrame:\n",
        "    # generate predictions from each model\n",
        "    predictions = pd.DataFrame(index=live_features.index)\n",
        "    for target in favorite_targets:\n",
        "        predictions[target] = models[target].predict(live_features[feature_cols])\n",
        "    # ensemble predictions\n",
        "    ensemble = predictions.rank(pct=True).mean(axis=1)\n",
        "    # format submission\n",
        "    submission = ensemble.rank(pct=True, method=\"first\")\n",
        "    return submission.to_frame(\"prediction\")\n",
        "\n",
        "# Quick test\n",
        "napi.download_dataset(\"v4.2/live_int8.parquet\")\n",
        "live_features = pd.read_parquet(f\"v4.2/live_int8.parquet\", columns=feature_cols)\n",
        "predict_ensemble(live_features)\n",
        "\n",
        "# Use the cloudpickle library to serialize your function and its dependencies\n",
        "import cloudpickle\n",
        "p = cloudpickle.dumps(predict_ensemble)\n",
        "with open(\"predict_ensembleTodayTxr.pkl\", \"wb\") as f:\n",
        "    f.write(p)\n",
        "    \n",
        "    # Download file if running in Google Colab\n",
        "try:\n",
        "        from google.colab import file\n",
        "        file('predict_ensemble.pkl')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
